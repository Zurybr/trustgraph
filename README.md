# TrustGraph - Memoria del Workspace

Sistema Operativo de Contexto basado en grafos de conocimiento para el workspace. Transforma la documentaciÃ³n existente en un grafo de contexto inteligente que puede ser consultado usando GraphRAG.

## ğŸ¯ InstalaciÃ³n RÃ¡pida con Skill (Recomendado)

Instala TrustGraph como una skill de AI agent para acceso instantÃ¡neo:

```bash
# Buscar la skill
npx skills find trustgraph

# Instalar la skill
npx skills add tu-usuario/trustgraph
```

Una vez instalada, Kimi reconocerÃ¡ automÃ¡ticamente comandos como:
- "Inicia TrustGraph"
- "Carga documentaciÃ³n en TrustGraph"
- "Consulta TrustGraph sobre..."

Para una guÃ­a paso a paso detallada, ver [README_DUMMIES.md](README_DUMMIES.md).

## ğŸš€ Quick Start (Manual)

### OpciÃ³n A - Setup con Wizard Interactivo (Recomendado)

```bash
# 1. ConfiguraciÃ³n interactiva con menÃºs navegables
./setup.sh makeenv
# â†‘â†“ para navegar, ENTER para seleccionar proveedor

# 2. Iniciar TrustGraph
make up

# 3. Cargar documentaciÃ³n
make load

# 4. Acceder al Workbench
open http://localhost:8888
```

### OpciÃ³n B - Setup Manual

```bash
# 1. Configurar variables de entorno
cp .env.example .env
# Editar .env con tus API keys manualmente

# 2-4. Igual que OpciÃ³n A
make up && make load
```

## ğŸ“ Estructura del Proyecto

```
trustgraph/
â”œâ”€â”€ docker-compose.yaml          # ConfiguraciÃ³n de servicios
â”œâ”€â”€ .env.example                 # Variables de entorno de ejemplo
â”œâ”€â”€ README.md                    # Esta documentaciÃ³n
â”œâ”€â”€ config/                      # Configuraciones
â”‚   â”œâ”€â”€ context-core.yaml       # ConfiguraciÃ³n del knowledge base
â”‚   â”œâ”€â”€ garage.toml             # Object storage config
â”‚   â”œâ”€â”€ prometheus.yml          # MÃ©tricas
â”‚   â”œâ”€â”€ loki.yml                # Logging
â”‚   â”œâ”€â”€ promtail.yml            # Log collector
â”‚   â””â”€â”€ grafana/                # Dashboards
â”œâ”€â”€ scripts/                     # Scripts de utilidad
â”‚   â”œâ”€â”€ load_docs.py            # Cargar documentaciÃ³n
â”‚   â””â”€â”€ query_graphrag.py       # Consultar memoria
â””â”€â”€ data/                        # Datos persistentes (gitignored)
    â”œâ”€â”€ cassandra/              # Graph database
    â”œâ”€â”€ qdrant/                 # Vector database
    â”œâ”€â”€ garage/                 # Object storage
    â”œâ”€â”€ pulsar/                 # Message broker
    â”œâ”€â”€ prometheus/             # MÃ©tricas
    â”œâ”€â”€ grafana/                # Dashboards
    â””â”€â”€ loki/                   # Logs
```

## ğŸ”§ Servicios

| Servicio | Puerto | DescripciÃ³n |
|----------|--------|-------------|
| Workbench UI | [8888](http://localhost:8888) | Interfaz web principal |
| API Gateway | [8080](http://localhost:8080) | API REST/WebSocket |
| Grafana | [3000](http://localhost:3000) | Dashboards (admin/admin) |
| Prometheus | [9090](http://localhost:9090) | MÃ©tricas |
| Qdrant | [6333](http://localhost:6333) | Vector DB API |

## ğŸ§  Uso

### Cargar DocumentaciÃ³n

```bash
# Cargar todo desde documentation/
python scripts/load_docs.py

# Cargar directorio especÃ­fico
python scripts/load_docs.py ../otros-docs

# Simular sin enviar (dry-run)
python scripts/load_docs.py --dry-run

# Filtrar por categorÃ­a
python scripts/load_docs.py --category trustgraph
```

### Consultar Memoria

```bash
# Pregunta simple
python scripts/query_graphrag.py "Â¿QuÃ© es TrustGraph?"

# Modo interactivo
python scripts/query_graphrag.py --interactive

# BÃºsqueda vectorial
python scripts/query_graphrag.py --search "instalaciÃ³n"

# Explorar grafo
python scripts/query_graphrag.py --graph --depth 3

# Listar context cores
python scripts/query_graphrag.py --cores
```

### API REST

```bash
# Health check
curl http://localhost:8080/api/v1/health

# GraphRAG Query
curl -X POST http://localhost:8080/api/v1/graphrag/query \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Â¿QuÃ© es TrustGraph?",
    "context_core": "documentation",
    "include_sources": true
  }'

# Vector Search
curl -X POST http://localhost:8080/api/v1/search/vector \
  -H "Content-Type: application/json" \
  -d '{
    "query": "arquitectura del sistema",
    "collection": "docs",
    "limit": 5
  }'
```

## âš™ï¸ ConfiguraciÃ³n

### Variables de Entorno

```bash
# LLM Provider: openai, anthropic, zai, kimi, minimax, ollama
LLM_PROVIDER=openai
OPENAI_API_KEY=sk-your-key

# O usar modelos chinos (Z.AI GLM, Kimi, MiniMax)
LLM_PROVIDER=zai
ZAI_API_KEY=your-zai-key

# O usar Ollama local
LLM_PROVIDER=ollama
OLLAMA_BASE_URL=http://host.docker.internal:11434

# ConfiguraciÃ³n del proyecto
CONTEXT_CORE_ID=documentation
COLLECTION_NAME=docs
```

### Cambio RÃ¡pido de Proveedor

**Wizard Interactivo (con menÃºs navegables):**
```bash
# ConfiguraciÃ³n completa con flechas â†‘â†“ para navegar
make makeenv
# o
./setup.sh makeenv
```

**Comando General RÃ¡pido:**
```bash
# Ver menÃº interactivo
make provider

# Cambiar directamente a cualquier proveedor
make provider USE=zai       # Z.AI (GLM-5)
make provider USE=kimi      # Kimi (K2)
make provider USE=minimax   # MiniMax (M2.5)
make provider USE=openai    # OpenAI (GPT-4o)
make provider USE=ollama    # Ollama local

# Reiniciar para aplicar cambios
docker compose restart
```

### Proveedores Soportados

| Proveedor | Modelos | Tipo API |
|-----------|---------|----------|
| OpenAI | GPT-4, GPT-4o, GPT-3.5 | OpenAI |
| Anthropic | Claude 3.5 Sonnet, Claude 3 Opus | Anthropic |
| **Z.AI (æ™ºè°±AI)** | **GLM-5, GLM-4.6V** | OpenAI-compatible |
| **Kimi** | **Kimi K2, Kimi Code** | Anthropic-compatible |
| **MiniMax** | **MiniMax-M2.5** | Anthropic-compatible |
| Ollama | llama3.1, qwen, etc. | Local |

### Recursos Requeridos

| Recurso | MÃ­nimo | Recomendado |
|---------|--------|-------------|
| CPU | 4 cores | 8+ cores |
| RAM | 8 GB | 16+ GB |
| Disco | 50 GB SSD | 100+ GB SSD |

## ğŸ³ Comandos Docker

```bash
# Iniciar todo
docker compose up -d

# Ver logs
docker compose logs -f

# Ver logs de un servicio
docker compose logs -f graphrag

# Escalar servicio
docker compose up -d --scale graphrag=3

# Detener todo
docker compose down

# Detener y eliminar datos
docker compose down -v

# Reconstruir
docker compose up -d --build
```

## ğŸ“Š Monitoreo

- **Grafana**: http://localhost:3000 (admin/admin)
- **Prometheus**: http://localhost:9090
- **MÃ©tricas**: LLM latency, error rates, token throughput

## ğŸ”„ Pipeline GraphRAG

```
Documento â†’ Text Load â†’ Knowledge Builder â†’ Triples â†’ Cassandra
                                    â†“
                            Embeddings â†’ Qdrant

Query â†’ GraphRAG â†’ Vector Search â†’ Graph Traversal â†’ Context Assembly â†’ Response
```

## ğŸ› ï¸ Troubleshooting

### TrustGraph no responde

```bash
# Verificar servicios
docker compose ps

# Ver logs
docker compose logs -f workbench

# Reiniciar
docker compose restart
```

### Error de API Key

```bash
# Verificar variable
echo $OPENAI_API_KEY

# Recargar .env
source .env

# Reiniciar servicios
docker compose restart
```

### Puerto ocupado

```bash
# Encontrar proceso
lsof -i :8888

# Matar proceso o cambiar puerto en docker-compose.yaml
```

## ğŸ“š DocumentaciÃ³n

- [TrustGraph Docs](https://docs.trustgraph.ai)
- [TrustGraph GitHub](https://github.com/trustgraph-ai/trustgraph)
- [API Reference](../documentation/trustgraph/api-reference.md)

## ğŸ¤ IntegraciÃ³n con Claude Code

TrustGraph puede integrarse con Claude Code a travÃ©s de MCP:

```json
// ~/.claude/settings.json
{
  "mcpServers": {
    "trustgraph": {
      "command": "python",
      "args": ["/path/to/trustgraph/scripts/mcp_server.py"]
    }
  }
}
```

Ver [integracion-claude.md](../documentation/ecosystem/trustgraph/integracion-claude.md) para mÃ¡s detalles.

## ğŸ“ Notas

- Los datos se almacenan en `data/` (persistentes entre reinicios)
- Para producciÃ³n, usa Kubernetes y configura backups
- La primera carga de documentos puede tardar varios minutos
- Usa `--dry-run` para probar antes de cargar

## ğŸ†˜ Soporte

- GitHub Issues: https://github.com/trustgraph-ai/trustgraph/issues
- Discord: https://discord.gg/trustgraph
