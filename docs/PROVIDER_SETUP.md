# Gu√≠a de Configuraci√≥n de Proveedores LLM

TrustGraph soporta m√∫ltiples proveedores de LLM, incluyendo modelos chinos (Z.AI, Kimi, MiniMax) y modelos occidentales (OpenAI, Anthropic).

## üöÄ M√©todos de Configuraci√≥n

### 1. Wizard Interactivo (Recomendado)

El m√©todo m√°s f√°cil y r√°pido para configurar tu proveedor:

```bash
# Usando make (recomendado)
make makeenv

# Usando el script de setup
./setup.sh makeenv

# Directamente con Python
python3 scripts/setup_env.py
```

**Caracter√≠sticas del wizard:**
- ‚úÖ Navegaci√≥n con flechas ‚Üë‚Üì
- ‚úÖ Descripci√≥n detallada de cada proveedor
- ‚úÖ Validaci√≥n de API keys
- ‚úÖ Confirmaci√≥n de modelos
- ‚úÖ Compatible con todas las terminales

### 2. Comando General R√°pido

Para cambiar de proveedor despu√©s de la configuraci√≥n inicial:

```bash
# Ver men√∫ interactivo
make provider

# Cambiar directamente
make provider USE=zai
make provider USE=kimi
make provider USE=minimax
make provider USE=openai
make provider USE=anthropic
make provider USE=ollama
```

### 3. Configuraci√≥n Manual

Para usuarios avanzados que prefieren editar directamente:

```bash
# Editar el archivo .env
nano .env
```

## üìã Proveedores Soportados

### Z.AI (Êô∫Ë∞±AI / GLM)

**Modelos disponibles:**
- `glm-5` - Modelo flagship (recomendado)
- `glm-4.6v` - Multimodal con visi√≥n

**Endpoints:**
- General: `https://api.z.ai/api/paas/v4`
- Coding: `https://api.z.ai/api/coding/paas/v4`

**Configuraci√≥n:**
```bash
LLM_PROVIDER=zai
ZAI_API_KEY=your-api-key
ZAI_BASE_URL=https://api.z.ai/api/paas/v4
ZAI_MODEL=glm-5
```

**Obtener API key:** [Z.AI Open Platform](https://open.bigmodel.cn/usercenter/apikeys)

---

### Kimi (Moonshot AI)

**Modelos disponibles:**
- `kimi-k2` - Modelo principal
- `kimi-for-coding` - Optimizado para coding

**Endpoint:**
- `https://api.kimi.com/coding`

**Configuraci√≥n:**
```bash
LLM_PROVIDER=kimi
KIMI_API_KEY=sk-kimi-your-key
KIMI_BASE_URL=https://api.kimi.com/coding
KIMI_MODEL=kimi-k2
```

**Compatibilidad:** Usa formato API Anthropic

**Obtener API key:** [Kimi Platform](https://platform.moonshot.cn/console/api-keys)

---

### MiniMax

**Modelos disponibles:**
- `MiniMax-M2.5` - Modelo principal

**Endpoints:**
- Internacional: `https://api.minimax.io/anthropic`
- China: `https://api.minimaxi.com/anthropic`

**Configuraci√≥n:**
```bash
LLM_PROVIDER=minimax
MINIMAX_API_KEY=your-api-key
MINIMAX_BASE_URL=https://api.minimax.io/anthropic
MINIMAX_MODEL=MiniMax-M2.5
```

**Compatibilidad:** Usa formato API Anthropic

**Obtener API key:** [MiniMax Platform](https://www.minimaxi.com/platform/settings/api-keys)

---

### OpenAI

**Modelos disponibles:**
- `gpt-4o` - Modelo recomendado
- `gpt-4-turbo`
- `gpt-3.5-turbo`

**Configuraci√≥n:**
```bash
LLM_PROVIDER=openai
OPENAI_API_KEY=sk-your-key
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_MODEL=gpt-4o
```

**Obtener API key:** [OpenAI Platform](https://platform.openai.com/api-keys)

---

### Anthropic

**Modelos disponibles:**
- `claude-3-5-sonnet-20241022` - Recomendado
- `claude-3-opus-20240229`
- `claude-3-haiku-20240307`

**Configuraci√≥n:**
```bash
LLM_PROVIDER=anthropic
ANTHROPIC_API_KEY=sk-ant-your-key
ANTHROPIC_BASE_URL=https://api.anthropic.com
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022
```

**Obtener API key:** [Anthropic Console](https://console.anthropic.com/settings/keys)

---

### Ollama (Local)

**Modelos disponibles:**
- `llama3.1`
- `qwen`
- `mistral`
- Cualquier modelo de Ollama

**Configuraci√≥n:**
```bash
LLM_PROVIDER=ollama
OLLAMA_BASE_URL=http://host.docker.internal:11434
OLLAMA_MODEL=llama3.1
```

**Requisitos:** Tener Ollama instalado y ejecut√°ndose localmente.

**Descargar:** [Ollama](https://ollama.com/download)

---

## üîÑ Cambio Entre Proveedores

### Despu√©s de Cambiar el Proveedor

Siempre reinicia TrustGraph para aplicar los cambios:

```bash
# M√©todo 1 - Reinicio completo
make down && make up

# M√©todo 2 - Reinicio r√°pido
docker compose restart

# M√©todo 3 - Reinicio solo de servicios LLM
docker compose restart graph-rag doc-embeddings
```

### Verificar Configuraci√≥n Actual

```bash
# Ver proveedor actual
make provider

# O directamente
grep LLM_PROVIDER .env
```

## üõ†Ô∏è Troubleshooting

### Error: "API key no v√°lida"

1. Verifica que la API key est√© en el formato correcto
2. Aseg√∫rate de que la key no haya expirado
3. Confirma que tienes saldo/cr√©ditos en la plataforma

### Error: "No se puede conectar al endpoint"

1. Verifica tu conexi√≥n a internet
2. Si usas endpoint de China (Z.AI, Kimi, MiniMax), puede requerir VPN desde ciertos pa√≠ses
3. Verifica que el BASE_URL sea correcto

### Error: "Modelo no encontrado"

1. Verifica que el nombre del modelo sea exacto
2. Algunos modelos pueden tener diferentes nombres en la API vs la documentaci√≥n

### Problemas con Ollama

1. Aseg√∫rate de que Ollama est√© ejecut√°ndose: `ollama serve`
2. Verifica que el modelo est√© descargado: `ollama pull llama3.1`
3. En Docker Desktop, usa `host.docker.internal` para conectar al host

## üí° Mejores Pr√°cticas

1. **Primera vez:** Usa `make makeenv` para configurar f√°cilmente
2. **Pruebas:** Comienza con proveedores que tengan free tier (Kimi, Z.AI)
3. **Producci√≥n:** Considera usar m√∫ltiples proveedores como fallback
4. **Costos:** Monitorea el uso en los dashboards de cada proveedor

## üìä Comparaci√≥n de Proveedores

| Proveedor | Latencia | Calidad | Precio | Contexto |
|-----------|----------|---------|--------|----------|
| OpenAI GPT-4o | Media | Alta | $$$ | 128K |
| Claude 3.5 | Media | Alta | $$$ | 200K |
| GLM-5 | Baja | Alta | $ | 128K |
| Kimi K2 | Baja | Alta | $ | 200K |
| MiniMax | Baja | Media | $ | 200K |
| Ollama | Muy baja | Variable | Gratis | Variable |

---

**¬øNecesitas ayuda?** Usa `make provider` para ver el men√∫ interactivo o consulta la documentaci√≥n en `CLAUDE.md`.
